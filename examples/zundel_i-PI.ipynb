{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Gaussian approximation potential (GAP) for  the Zundel cation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The present notebook is meant to give you an overview of the main ingredients that you need to build an interatomic potential with `librascal` and use it in connection with i-Pi (https://github.com/cosmo-epfl/i-pi) to generate molecular dynamics (MD) trajectories of the system of interest. \n",
    "We will start from building a GAP model for Zundel cations ($H_5O_2+$), using a training set obtained via Bowman PES sampling, calculate its RMSE on a test set to check its performance and run a short NVT simulation at $\\text{T} = 250\\,\\text{K}$. \n",
    "\n",
    "The mathematical framework that we are going to use is the kernel-GAP fitting method, using both total energies and atomic forces as target properties. Basically the GAP-model total energy of a zundel molecule is computed using the following expression: \n",
    "\n",
    "$$\n",
    "                    E = E_0 + \\sum_i \\sum_{s=1}^M \\alpha_s K(\\bf{d_i} , \\bf{d_s} )                   \n",
    "$$\n",
    "\n",
    "where $E_0$ represents an energy baseline (given usually by the sum of atomic self-contributions), $\\bf{d_i}$ ($i = 1, \\dots, N_{atoms}$) are the set of (normalized SOAP) descriptors describing an environment centred around atom $i$, $\\bf{d_s}$ the set of descriptors corresponding to the environments of the sparse set (of size $M$) and K a kernel matrix that describes the similarity of two different atomic environments. In our application, the kernel is just the dot product, raised to some integer power $\\zeta$: \n",
    "\n",
    "$$\n",
    "        K(\\bf{d_i} , \\bf{d_s} ) \\propto \\left| \\bf{d_i} \\cdot \\bf{d_s} \\right|^{\\zeta}           \n",
    "$$\n",
    "\n",
    "Finally $\\alpha_s$ represents the weights of each sparse environment, to be determined using Kernel-Ridge Regression (KRR). For extensive details on the SOAP GAP-model fitting procedure and interesting physical applications, we invite the reader to refer to the book chapter [1].\n",
    "Details on the implementation in `librascal` are instead given in the companion publication [2].\n",
    "\n",
    "[1]: M. Ceriotti, M.J. Willatt, and G. Csányi, in Handbook of Materials Modeling (Springer International Publishing, Cham, 2018), pp. 1–27. https://doi.org/10.1007/978-3-319-42913-7_68-1\n",
    "\n",
    "[2]: F. Musil, M. Veit, A. Goscinski, G. Fraux, M.J. Willatt, M. Stricker, T. Junge, and M. Ceriotti, J. Chem. Phys. 154, 114109 (2021). https://doi.org/10.1063/5.0044689"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to be able to fit a potential with `librascal` (the model evaluator) and interface it with i-Pi (the MD engine) we first need to have both softwares available and correctly installed. For `librascal`, the following should suffice:\n",
    "```bash\n",
    "$ pip install git+https://github.com/cosmo-epfl/librascal.git\n",
    "```\n",
    "\n",
    "and the same for i-PI:\n",
    "```bash\n",
    "$ pip install git+https://github.com/cosmo-epfl/i-pi.git\n",
    "```\n",
    "If these steps don't work, try looking at the `README.rst` files of the respective repositories for further instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing all the necessary Librascal modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start from the import of all the necessary modules and classes. In `librascal`, these are: \n",
    "\n",
    "1) the SOAP descriptors of a structure, by means of the `SphericalInvariants` class;\n",
    "\n",
    "2) the kernels between a set of environments and the sparse set (the `Kernel` class);\n",
    "\n",
    "3) the GAP model itself, which is saved as an object of the `KRR` class. The predict method of the same class will allow us to give predictions to new unseen structures. \n",
    "\n",
    "NOTE: it is assumed that the input format for the training set and the test set is an ASE-compatible (e.g. extended-XYZ) file, which is then converted into an array of ASE Atoms objects, each one corresponding to a specific structure. `librascal` uses these frames to compute the representations and predict the properties of new structures. In this example on zundel cations, the target energies to build the model with are reported in the `zundel_energies.txt` file, while the atomic forces are additional columns of the xyz. \n",
    "\n",
    "Alternatively, global target properties can be reported in the header line following the ASE format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pylab as plt\n",
    "\n",
    "import ase\n",
    "from ase.io import read, write\n",
    "from ase.build import make_supercell\n",
    "from ase.visualize import view\n",
    "import numpy as np\n",
    "# If installed -- not essential, though\n",
    "try:\n",
    "    from tqdm.notebook import tqdm\n",
    "except ImportError:\n",
    "    tqdm = (lambda i, **kwargs: i)\n",
    "\n",
    "from time import time\n",
    "\n",
    "from rascal.models import Kernel, train_gap_model, compute_KNM\n",
    "from rascal.representations import SphericalInvariants\n",
    "from rascal.utils import from_dict, to_dict, CURFilter, dump_obj, load_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd i-PI/zundel/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the snippets below we extract the relevant properties for each ASE frame. We load the total potential energies and we use the `ASE.Atoms.arrays` methods to get the atomic forces. For more information on how to use ASE-related methods, check the [ASE Atoms documentation](https://wiki.fysik.dtu.dk/ase/ase/atoms.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the first N structures of the zundel dataset\n",
    "N_dataset = 1000\n",
    "frames = read('zundel_dataset.xyz', index=':{}'.format(N_dataset))\n",
    "energies = np.loadtxt('zundel_energies.txt')[:N_dataset] \n",
    "\n",
    "#Keys of the arrays dictionary\n",
    "print(frames[0].arrays.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def extract_forces(frames,array_key='zeros'):\n",
    "    f = []\n",
    "    for frame in frames:\n",
    "        if array_key is None:\n",
    "            pass\n",
    "        elif array_key == 'zeros':\n",
    "            f.append(np.zeros(frame.get_positions().shape))\n",
    "        else:\n",
    "            f.append(frame.get_array(array_key))\n",
    "    try:\n",
    "        f = np.concatenate(f)\n",
    "    except:\n",
    "        pass\n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the GAP model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preliminary step towards building a GAP model is defining a training and a test set. The training set is built by random selection of $800$ of the available structures, while the remaining $200$ will represent the test set. At this point, we also extract the distinct atomic species present in our system, define the atomic baseline energy and extract the information regarding energies and forces in the train and test set structures, which we will need later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Number of structures to train the model with\n",
    "n = 800\n",
    "\n",
    "global_species = set()\n",
    "for frame in frames:\n",
    "    global_species.update(frame.get_atomic_numbers())\n",
    "global_species = np.array(list(global_species))\n",
    "\n",
    "# Select randomly n structures for training the model\n",
    "ids = list(range(N_dataset))\n",
    "np.random.seed(10)\n",
    "np.random.shuffle(ids)\n",
    "\n",
    "train_ids = ids[:n]\n",
    "frames_train = [frames[ii] for ii in ids[:n]]\n",
    "y_train = [energies[ii] for ii in ids[:n]]\n",
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_train = extract_forces(frames_train, 'forces')\n",
    "f = extract_forces(frames, 'forces')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Atomic energy baseline\n",
    "atom_energy_baseline = np.mean(energies)/(frames[0].get_global_number_of_atoms())\n",
    "energy_baseline = {np.int(species): atom_energy_baseline for species in global_species}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we proceed with the actual calculation of the SOAP vectors of our training set. We need to specify an hyperparameters dictionary, which `librascal` uses to compute the structural features. The meaning of each hyperparameter and how to correctly set them is reported in [the `SphericalInvariants` documentation](https://cosmo-epfl.github.io/librascal/reference/python.html#rascal.representations.SphericalInvariants). These hyperparameters can be used as default values, but a careful optimization of the interaction cutoff might be required in the case the material under investigation might present some mid- or long-range order. \n",
    "\n",
    "For the actual calculation of the SOAP features, we first create an object of the `SphericalInvariants` class, defined by its hyperparameters. The methods that we then need to use are `transform()}`, which yields a second object called the `manager` containing the representation, while `get_features()` converts it into an $NxM$ matrix, $N$ being the number of atomic environments in the training set and M the number of features per each environment. \n",
    "\n",
    "At this point, the `compute_gradients` hyper should be set to `False`. For now, we will only use the SOAP representations (no gradients!) of the training set structures to select a sparse set using the CUR decomposition. Setting it to `True` would require Librascal to compute all the gradients of the SOAP representation w.r.t. atomic coordinates, thus making the structural managers unnecessarily memory expensive. We will instead compute later on the gradients of the sparse kernels using the `compute_KNM` method when we fit the GAP model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the parameters of the spherical expansion\n",
    "hypers = dict(soap_type=\"PowerSpectrum\",\n",
    "              interaction_cutoff=3.0, \n",
    "              max_radial=8, \n",
    "              max_angular=6, \n",
    "              gaussian_sigma_constant=0.5,\n",
    "              gaussian_sigma_type=\"Constant\",\n",
    "              cutoff_function_type=\"RadialScaling\",\n",
    "              cutoff_smooth_width=0.5,\n",
    "              cutoff_function_parameters=\n",
    "                    dict(\n",
    "                            rate=1,\n",
    "                            scale=3.5,\n",
    "                            exponent=4\n",
    "                        ),\n",
    "              radial_basis=\"GTO\",\n",
    "              normalize=True,\n",
    "              optimization=\n",
    "                    dict(\n",
    "                            Spline=dict(\n",
    "                               accuracy=1.0e-05\n",
    "                            )\n",
    "                        ),\n",
    "              compute_gradients=False\n",
    "              )\n",
    "\n",
    "\n",
    "soap = SphericalInvariants(**hypers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following snippet computes the descriptors and the managers for each frame, which will represent the expensive part of the calculation, in terms of memory usage.\n",
    "\n",
    "NOTE: the `librascal` structure manager only works if the atoms have been wrapped within the cell provided in the input file using e.g. `ase.Atoms.wrap()`, or one of the structure preprocessors provided in `rascal.neighbourlist.structure_manager` module (useful especially for non-periodic structures)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "managers = []\n",
    "for f in tqdm(frames_train):\n",
    "    f.wrap(eps=1e-18)\n",
    "\n",
    "start = time()\n",
    "managers = soap.transform(frames_train)\n",
    "print (\"Execution: \", time()-start, \"s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we can define the **sparse set**, i.e. the set of (ideally) \"maximally diverse\" environments that will be used as basis points of the kernel ridge regression (KRR, mostly equivalent to GAP) model. We can choose the number of sparse environments in `librascal` per each atomic species, by defining a dictionary containing (atomic number, number of environments) pairs. The CURFilter class then selects out the most \"representative\" environments according to the representation that we provide as the input object and by applying the CUR decomposition [3].\n",
    "\n",
    "[3]: 1 M.W. Mahoney and P. Drineas, PNAS 106, 697 (2009). https://doi.org/10.1073/pnas.0803205106"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# select the sparse points for the sparse kernel method with CUR on the whole training set\n",
    "n_sparse = {1:250, 8:125}\n",
    "compressor = CURFilter(soap, n_sparse, act_on='sample per species')\n",
    "X_sparse = compressor.select_and_filter(managers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in eq. $(1)$, in order to fit a GAP potential we need to compute the kernels of all the training structure descriptors and the sparse set, as well as the gradients w.r.t. the atomic positions, if we wish to fit the atomic forces. We do so by using another key structure of librascal: the `Kernel` class. We first build a kernel object containing the representation (SOAP), which we then use to compute the sparse kernel matrix $K_{NM}$ between the training set and the sparse set and its gradients using the `compute_KNM` method.\n",
    "\n",
    "Finally, we group everything together to build the GAP model, which basically uses the kernel matrices to regress the weights $\\{\\alpha_s\\}_{s=1}^M$ on each sparse environment. The `lambdas` are the regularization parameters of the KRR (for both energies and forces), while `jitter` is a small parameter that enables its numerical convergence. \n",
    "\n",
    "The output is a KRR object, which we can save as .json file for future use. For this last bit we use the `dump_obj` method (part of the `rascal.utils.io` module)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeta = 2\n",
    "\n",
    "start = time()\n",
    "hypers['compute_gradients'] = True\n",
    "soap = SphericalInvariants(**hypers)\n",
    "kernel = Kernel(soap, name='GAP', zeta=zeta, target_type='Structure', kernel_type='Sparse')\n",
    "\n",
    "KNM = compute_KNM(tqdm(frames_train, leave=True, desc=\"Computing kernel matrix\"), X_sparse, kernel, soap)\n",
    "\n",
    "model = train_gap_model(kernel, frames_train, KNM, X_sparse, y_train, energy_baseline, \n",
    "                        grad_train=-f_train, lambdas=[1e-12, 1e-12], jitter=1e-13)\n",
    "\n",
    "# save the model to a file in json format for future use\n",
    "dump_obj('zundel_model.json', model)\n",
    "np.savetxt('Structure_indices.txt', ids)\n",
    "print (\"Execution: \", time()-start, \"s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessment of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform a basic assessment of the model we have fitted, we need to predict the properties (energies and forces) of a test set. In the calculation of the model that we have fitted previously, we have randomly splitted the original dataset in a training set of 800 structures and a test set of another 200.\n",
    "So first we load the model and the indices corresponding to the shuffled dataset: the first 800 of those will correspond to the training set, the last 200 to the test set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 800\n",
    "model = load_obj('zundel_model.json')\n",
    "ids = np.loadtxt('Structure_indices.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = [np.int(i) for i in ids[:n]]\n",
    "test_ids = [np.int(i) for i in ids[n:]] \n",
    "\n",
    "frames_train = [frames[ii] for ii in train_ids]\n",
    "frames_test = [frames[ii] for ii in test_ids]\n",
    "\n",
    "y_train = [energies[ii] for ii in train_ids]\n",
    "y_train = np.array(y_train)\n",
    "y_test = [energies[ii] for ii in test_ids]\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now compute the predictions on the test set, using the `predict()` and `predict_forces()` methods of the `KRR` class. At this point we need to compute the SOAP representation of the test set structures. It is important to stress however that this will not cause any issue regarding memory usage, because all we need is the predictions, so we can compute the managers of the test set structures one by one and calculate the predictions right away. Instead for the GAP fitting we need to store ALL the structure managers of the training set to perform the regression (which causes a large RAM usage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions on the test set\n",
    "y_pred = []\n",
    "f_pred = []\n",
    "\n",
    "for f in tqdm(frames_test):\n",
    "    positions = f.get_positions()\n",
    "    f.set_positions(positions+[1,1,1])\n",
    "    f.wrap(eps=1e-18)\n",
    "    m = soap.transform(f)\n",
    "    y_pred.append(model.predict(m))\n",
    "    f_pred.append(model.predict_forces(m))\n",
    "\n",
    "y_pred = np.array(y_pred).flatten()\n",
    "f_pred = np.array(f_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_test = extract_forces(frames_test, 'forces')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now compute the overall performance of the model. This is done by calculation of the Root-Mean-Square Error (RMSE), i.e. the standard deviation of the residuals according to the standard formula:\n",
    "\n",
    "$$\n",
    "       \\text{RMSE} = \\sqrt{\\frac{1}{n_{\\text{test}}}\\sum_i (y_{\\text{pred}}^{i} - y_{\\text{test}}^{i})^2}\n",
    "$$\n",
    "   \n",
    "which we can compare to the standard deviation of the test set itself to quantify how much the model captures the energy variations in the test set. The $\\% \\text{RMSE}$ of our model is about $5 \\%$ of the training set STD, which is sufficiently accurate to run MD safely. \n",
    "\n",
    "Finally we plot a \"correlation plot\", to observe how well the predictions on the test set correlate with the reference DFT-computed energies.\n",
    "\n",
    "NOTE: by just repeating this procedure and fitting potentials with increasing training set size, one can also construct the learning curve of the GAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rascal.utils import get_score\n",
    "\n",
    "score = get_score(y_pred, y_test)\n",
    "RMSE = score['RMSE']\n",
    "sigma_test = np.std(y_test)\n",
    "print(\"RMSE = \", RMSE*1000.0, \"meV\")\n",
    "print(\"Sigma test set = \", sigma_test, \" eV\")\n",
    "print(\"%RMSE = \", RMSE/sigma_test*100.0, \" %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test, y_pred, s=5.0, c = 'black')\n",
    "lims = [\n",
    "    np.min([np.min(y_pred), np.min(y_test)]),  # min of both axes\n",
    "    np.max([np.max(y_pred), np.max(y_test)]),  # max of both axes\n",
    "]\n",
    "\n",
    "# now plot both limits against eachother\n",
    "plt.plot(lims, lims, 'k-', zorder=0, color='red')\n",
    "plt.title(\"correlation plot\")\n",
    "plt.xlabel(\"predicted energies [eV]\")\n",
    "plt.ylabel(\"reference energies [eV]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MD simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to use the fitted model to perform a simple NVT simulation at $\\text{T} = 250\\,$K using the i-Pi interface with librascal. For that we will use a communication socket run by i-Pi, which basically outputs a structure produced by the MD and gives it in input to Librascal. This will in turn return energies, forces and stresses by means of the `GenericMDCalculator` class of Librascal. \n",
    "\n",
    "The job itself will generate a parent process (i-Pi) which contains information of the ensemble, the step needed for the time-integration, the thermostat characteristics, and all other trajectory-related infos. All these information are initially stored in an input.xml file as specified in the i-Pi documentation at https://github.com/cosmo-epfl/i-pi and given as inputs to i-Pi. The Librascal calculator is then launched as a child process and exchanges information with the MD driver.  \n",
    "\n",
    "The Librascal driver in i-Pi needs some input parameters, that can be given directly in the command line when the driver is called. To check the needed information, just use the --help option when calling it, as shown below.\n",
    "\n",
    "NOTE: in what follows, it is assumed that your i-Pi and Librascal folders lie in a common directory. Check and modify the path below when defining IPI if this is not the case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the XML file in the `i-PI/zundel` folder to quickly read the relevant information about the MD settings. Importantly, the `<properties>` tag gives you information about the physical properties that are printed out by i-PI. This simulation gives as output a .out file containing the time-evolution of all the relevant physical quantities, while the .xc.xyz file contains the full trajectory, which you can later visualize with VMD. The file `h5o2+.xyz` is both used by i-PI as a starting configuration of the trajectory and by Librascal as a template to load information about chemical species and number of atoms per species of the system.\n",
    "\n",
    "As the simulation evolves you can plot some interesting physical properties, for instance the MD kinetic energy, the total potential energy and the pressure, and check for thermalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example script is provided to launch a MD simulation with i-PI and the model that has just been fitted `zundel_model.json` (note that you may need to edit some variables in the script, or set `$PATH` and `$PYTHONPATH` appropriately, in order to make it work on your system):\n",
    "\n",
    "```bash\n",
    "bash ./run.sh\n",
    "```\n",
    "\n",
    "and the simulation can be monitored with the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the results of the simulations\n",
    "try:\n",
    "    f = open('zundel.out', 'r')\n",
    "except FileNotFoundError:\n",
    "    f = open('zundel.example.out', 'r')\n",
    "lines=f.readlines()[13:]\n",
    "\n",
    "Nframes = len(lines)\n",
    "steps = np.zeros((Nframes, 1), dtype=float)\n",
    "time = np.zeros((Nframes, 1), dtype=float)\n",
    "KE = np.zeros((Nframes, 1), dtype=float)\n",
    "PE = np.zeros((Nframes, 1), dtype=float)\n",
    "Pressure = np.zeros((Nframes, 1), dtype=float)\n",
    "\n",
    "i = 0\n",
    "for x in lines: \n",
    "    steps[i] = float(x.split()[0])\n",
    "    time[i] = float(x.split()[1])\n",
    "    KE[i] = float(x.split()[4])\n",
    "    PE[i] = float(x.split()[5])\n",
    "    Pressure[i] = float(x.split()[6])\n",
    "    i += 1\n",
    "\n",
    "f, axs = plt.subplots(2, 1, sharex=True)\n",
    "f.subplots_adjust(hspace=0)\n",
    "f.suptitle('T = 250 K', fontweight='bold')\n",
    "\n",
    "axs[0].plot(time, (KE - KE[0]), linewidth = 1.5, label = r'$\\Delta PE$')\n",
    "axs[0].plot(time, (PE - PE[0]), linewidth = 1.5, label = r'$\\Delta KE$')\n",
    "axs[0].set(ylabel= 'Energy [eV]')\n",
    "axs[0].set_ylim(-0.5, 0.5)\n",
    "axs[0].legend(ncol = 2, mode='expand', prop={'size': 12}, loc=\"lower center\")\n",
    "\n",
    "axs[1].plot(time, Pressure, linewidth = 1.5, color = 'red')\n",
    "axs[1].set(ylabel= 'MD Pressure [bar]')\n",
    "axs[1].set(xlabel= 'time [ps]')"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Initialization Cell",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
